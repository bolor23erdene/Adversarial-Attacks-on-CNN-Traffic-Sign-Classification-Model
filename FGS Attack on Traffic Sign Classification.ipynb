{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import skimage.data\n",
    "import skimage.transform\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"Loads a data set and returns two lists:\n",
    "    \n",
    "    images: a list of Numpy arrays, each representing an image.\n",
    "    labels: a list of numbers that represent the images labels.\n",
    "    \"\"\"\n",
    "    # Get all subdirectories of data_dir. Each represents a label.\n",
    "    directories = [d for d in os.listdir(data_dir) \n",
    "                   if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    # Loop through the label directories and collect the data in\n",
    "    # two lists, labels and images.\n",
    "    labels = []\n",
    "    images = []\n",
    "    for d in directories:\n",
    "        label_dir = os.path.join(data_dir, d)\n",
    "        file_names = [os.path.join(label_dir, f) \n",
    "                      for f in os.listdir(label_dir) if f.endswith(\".ppm\")]\n",
    "        # For each label, load it's images and add them to the images list.\n",
    "        # And add the label number (i.e. directory name) to the labels list.\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(int(d))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "# Load training and testing datasets.\n",
    "ROOT_PATH = \"/users/bold-erdenezolbayar/Desktop/github/tfSigns\"\n",
    "train_data_dir = os.path.join(ROOT_PATH, \"datasets/Training\")\n",
    "test_data_dir = os.path.join(ROOT_PATH, \"datasets/Testing\")\n",
    "\n",
    "images, labels = load_data(train_data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  (4575,) \n",
      "images:  (4575, 32, 32, 3)\n",
      "labels:  (4575, 62) \n",
      "images:  (4575, 32, 32, 3)\n",
      "(4575, 62)\n"
     ]
    }
   ],
   "source": [
    "images32 = [skimage.transform.resize(image, (32, 32), mode='constant') for image in images]\n",
    "# Prepare the training dataset\n",
    "labels_a = np.array(labels)\n",
    "images_a = np.array(images32)\n",
    "print(\"labels: \", labels_a.shape, \"\\nimages: \", images_a.shape)\n",
    "\n",
    "factor = 0.95\n",
    "tr = int(len(labels)*factor)\n",
    "\n",
    "X_dat = images_a\n",
    "Y_dat = np.zeros((len(labels),62))\n",
    "Y_dat[np.arange(len(labels)),labels] = 1\n",
    "print(\"labels: \", Y_dat.shape, \"\\nimages: \", X_dat.shape)\n",
    "\n",
    "X_train = X_dat#[:tr]\n",
    "y_train = Y_dat#[:tr]\n",
    "#X_valid = X_dat[tr:]\n",
    "#y_valid = Y_dat[tr:]\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:  (2520,) \n",
      "images:  (2520, 32, 32, 3)\n",
      "labels:  (2520, 62) \n",
      "images:  (2520, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test dataset.\n",
    "test_images, test_labels = load_data(test_data_dir)\n",
    "\n",
    "# Transform the images, just like we did with the training set.\n",
    "test_images32 = [skimage.transform.resize(image, (32, 32), mode='constant') for image in test_images]\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_images32 = np.array(test_images32)\n",
    "print(\"labels: \", test_labels.shape, \"\\nimages: \", test_images32.shape)\n",
    "\n",
    "X_test = test_images32\n",
    "y_test = np.zeros((len(test_labels),62))\n",
    "y_test[np.arange(len(test_labels)),test_labels] = 1\n",
    "print(\"labels: \", y_test.shape, \"\\nimages: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, logits=False, training=False):\n",
    "\n",
    "    with tf.variable_scope('conv0'):\n",
    "        z = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], padding='same', activation=tf.nn.relu)\n",
    "        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        z = tf.layers.conv2d(z, filters=64, kernel_size=[3, 3], padding='same', activation=tf.nn.relu)\n",
    "        z = tf.layers.max_pooling2d(z, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    with tf.variable_scope('flat'):\n",
    "        shape = z.get_shape().as_list()\n",
    "        z = tf.reshape(z, [-1, np.prod(shape[1:])])\n",
    "\n",
    "    with tf.variable_scope('dense'):\n",
    "        z = tf.layers.dense(z, units=128, activation=tf.nn.relu)\n",
    "        z = tf.layers.dropout(z, rate=0.25)\n",
    "\n",
    "    logits_ = tf.layers.dense(z, units=62, name='logits')\n",
    "    y = tf.nn.softmax(logits_, name='ybar')\n",
    "\n",
    "    if logits == True: #if logits is True\n",
    "        return y, logits_\n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgm(model, x, eps=0.01, epochs=1, sign=True, clip_min=0, clip_max=1):\n",
    "    xadv = tf.identity(x)\n",
    "    ybar = model(xadv) # returns 10x1 matrix, each number is between 0 and 1\n",
    "\n",
    "    indices = tf.argmax(ybar, axis=1) #returns max index\n",
    "    target = tf.one_hot(indices, 62, on_value=1.0, off_value=0.0)\n",
    "    eps = tf.abs(eps)\n",
    "\n",
    "    def _cond(xadv, i):\n",
    "        return tf.less(i, epochs)\n",
    "\n",
    "    def _body(xadv, i):\n",
    "        ybar, logits = model(xadv, logits=True)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=logits)\n",
    "        dy_dx, = tf.gradients(loss, xadv)\n",
    "        xadv = tf.stop_gradient(xadv + eps*tf.sign(dy_dx))\n",
    "        xadv = tf.clip_by_value(xadv, clip_min, clip_max)\n",
    "        return xadv, i+1\n",
    "\n",
    "    xadv, _ = tf.while_loop(_cond, _body, (xadv, 0), back_prop=False, name='fast_gradient')\n",
    "    return xadv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-3cac33e782e0>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph() \n",
    "\n",
    "class Environment():\n",
    "    pass\n",
    "\n",
    "env = Environment()\n",
    "\n",
    "with tf.variable_scope('model', reuse=False):\n",
    "    env.x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "    env.y = tf.placeholder(tf.float32, (None, 62), name='y')\n",
    "\n",
    "    env.ybar, logits = model(env.x, logits=True)\n",
    "\n",
    "    with tf.variable_scope('acc'):\n",
    "        count = tf.equal(tf.argmax(env.y, axis=1), tf.argmax(env.ybar, axis=1))\n",
    "        env.acc = tf.reduce_mean(tf.cast(count, tf.float32), name='acc')\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=env.y, logits=logits)\n",
    "        env.loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        env.train_op = optimizer.minimize(env.loss)\n",
    "\n",
    "with tf.variable_scope('model', reuse=True):\n",
    "    env.fgsm_eps = tf.placeholder(tf.float32, (), name='fgsm_eps')\n",
    "    env.fgsm_epochs = tf.placeholder(tf.int32, (), name='fgsm_epochs')\n",
    "    env.x_fgsm = fgm(model, env.x, epochs=env.fgsm_epochs, eps=env.fgsm_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(sess, env, X_data, Y_data, shuffle=True, batch=32, epochs=1):\n",
    "\tXshape = X_data.shape\n",
    "\tn_data = Xshape[0]\n",
    "\tn_batches = int(n_data/batch)\n",
    "\n",
    "\tprint(X_data.shape)\n",
    "\n",
    "\n",
    "\tfor ep in range(epochs):\n",
    "\t\tprint('epoch number: ', ep+1)\n",
    "\t\tif shuffle:\n",
    "\t\t\tind = np.arange(n_data)\n",
    "\t\t\tnp.random.shuffle(ind)\n",
    "\t\t\tX_data = X_data[ind]\n",
    "\t\t\tY_data = Y_data[ind]\n",
    "\n",
    "\t\tfor i in range(n_batches):\n",
    "\t\t\tprint(' batch {0}/{1}'.format(i + 1, n_batches), end='\\r')\n",
    "\t\t\tstart = i*batch \n",
    "\t\t\tend = min(start+batch, n_data)\n",
    "\n",
    "\t\t\tsess.run(env.train_op, feed_dict={env.x: X_data[start:end], env.y: Y_data[start:end]})\n",
    "\n",
    "\t\tevaluate(sess, env, X_test, y_test)\n",
    "\t\t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sess, env, X_test, Y_test, batch=32):\n",
    "\tn_data = X_test.shape[0]\n",
    "\tn_batches = int(n_data/batch)\n",
    "\n",
    "\ttotalAcc = 0\n",
    "\ttotalLoss = 0\n",
    "\n",
    "\n",
    "\tfor i in range(n_batches):\n",
    "\t\t#print('batch ', i)\n",
    "\t\tprint(' batch {0}/{1}'.format(i + 1, n_batches), end='\\r')\n",
    "\t\tstart = i*batch \n",
    "\t\tend = min(start+batch, n_data)\n",
    "\t\tbatch_X = X_test[start:end]\n",
    "\t\tbatch_Y = Y_test[start:end]\n",
    "\n",
    "\t\tbatch_loss, batch_acc = sess.run([env.loss, env.acc], feed_dict={env.x: batch_X, env.y: batch_Y})\n",
    "\n",
    "\t\ttotalAcc = totalAcc + batch_acc*(end-start)\n",
    "\t\ttotalLoss = totalLoss + batch_loss*(end-start)\n",
    "\t\t\n",
    "\n",
    "\ttotalAcc = totalAcc/n_data\n",
    "\ttotalLoss = totalLoss/n_data\n",
    "\tprint('acc: {0:.3f} loss: {1:.3f}'.format(totalAcc, totalLoss))\n",
    "\treturn totalAcc, totalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_fgsm(sess, env, X_data, epochs=1, eps=0.01, batch_size=32):\n",
    "    print('\\nGenerating adversarials via FGSM')\n",
    "\n",
    "    n_sample = X_data.shape[0]\n",
    "    n_batch = int((n_sample + batch_size - 1) / batch_size)\n",
    "    X_adv = np.empty_like(X_data)\n",
    "\n",
    "    for batch in range(n_batch):\n",
    "        print(' batch {0}/{1}'.format(batch + 1, n_batch), end='\\r')\n",
    "        start = batch * batch_size\n",
    "        end = min(n_sample, start + batch_size)\n",
    "        adv = sess.run(env.x_fgsm, feed_dict={\n",
    "            env.x: X_data[start:end],\n",
    "            env.fgsm_eps: eps,\n",
    "            env.fgsm_epochs: epochs})\n",
    "        X_adv[start:end] = adv\n",
    "    print()\n",
    "\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4575, 32, 32, 3)\n",
      "epoch number:  1\n",
      "acc: 0.640 loss: 1.418\n",
      "epoch number:  2\n",
      "acc: 0.813 loss: 0.648\n",
      "epoch number:  3\n",
      "acc: 0.838 loss: 0.528\n",
      "acc: 0.838 loss: 0.528\n",
      "\n",
      "Generating adversarials via FGSM\n",
      " batch 79/79\n",
      "acc: 0.218 loss: 3.870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.21825396825396826, 3.869977242606027)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "training(sess, env, X_train, y_train, shuffle=True, batch=32, epochs=3)\n",
    "evaluate(sess, env, X_test, y_test)\n",
    "\n",
    "X_adv = perform_fgsm(sess, env, X_test, eps=0.02, epochs=3)\n",
    "evaluate(sess, env, X_adv, y_test)\n",
    "\n",
    "\n",
    "\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
